{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Vision and Deep Learning \n",
    "## Lab 2.b &ndash; Stereo Camera Calibration Example\n",
    "This lab looks into calibrating stereo camera systems. it predominantly follows the same structure as 2.a, however we now look at pairs of images. To this end we want to utilise corresponding points within a set of image pairs to help identify intrinsic parameters linking the camera pairs in the real-world.\n",
    "\n",
    "There are two ways to approach this lab. You can either grab the calibration board from the front and grab a series of images from the KinectV2 using code similar to the first lab, or you can utilise the data I have captured myself. Both will then be used in the same way later on, but it may be nice to have a bit of personalisation for the data you will be using.\n",
    "\n",
    "First we need to obtain some images, identify a checkerboard pattern on the RGB image maps and then register the points identifed on the checkerboard across the different images. \n",
    "\n",
    "We will utilize OpenCV functionality to undertake a large portion of the pipeline, however I will also give a brief insight into the more hands-on approach that we can take (but this will be in MATLAB). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports <a id=\"imports\"></a>\n",
    "The following section defines the imports used for the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ndarray handling:\n",
    "import numpy as np\n",
    "\n",
    "# For plotting:\n",
    "%matplotlib notebook\n",
    "# Use this magic command if you can't interact with the plots.\n",
    "#%matplotlib ipympl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "plt.rcParams['figure.figsize'] = [9, 10]\n",
    "\n",
    "# For TCP/IP functionality:\n",
    "from struct import unpack\n",
    "import socket\n",
    "\n",
    "# For image processing applications\n",
    "import cv2\n",
    "\n",
    "# For saving images\n",
    "from PIL import Image\n",
    "\n",
    "# For operating-system tools and file-finding\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# For resizing images\n",
    "import skimage\n",
    "import skimage.transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Either grab calibration images from the Kinect over the network ...\n",
    "\n",
    "Execute this cell if you want to capture the images from the Kinect directly.\n",
    "\n",
    "This is an interactive capture. When you execute the cell, your input will be requested. Press Enter as many times as you want frames; every time you press enter, this code will send a request to the server to capture an image from the Kinect. When you are done, type \"q\".\n",
    "\n",
    "So team up with someone next to you&mdash;or steal the images while someone else is standing in front of the camera.\n",
    "\n",
    "When we are done capturing the images, we need to resize the retrieved data. When it is sent over the network, it is sent as a stream of bits. Therefore we need to reshape it back to its correct size on our end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders for the information from the Kinect.\n",
    "img = []\n",
    "dep = []\n",
    "infra = []\n",
    "\n",
    "# A function to get our messages over TCP/IP\n",
    "def getMessage(sock, data_type):\n",
    "    msg_size = sock.recv(8)\n",
    "    (msg_size,) = unpack('>Q', msg_size)\n",
    "    data = b''\n",
    "    while len(data) < msg_size:\n",
    "        left_to_read = msg_size - len(data)\n",
    "        data += sock.recv(8192 if left_to_read > 8192 else left_to_read)\n",
    "    data = np.frombuffer(data, data_type)\n",
    "    sock.send(b'1')\n",
    "    return data\n",
    "\n",
    "# Get data over TCP/IP\n",
    "server_ip = '137.44.6.201' # Edit this to reflect the correct IP address\n",
    "server_port = 20156\n",
    "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "s.connect((server_ip, server_port))\n",
    "\n",
    "n_frames = 0 # Do not change this unless you know what you are doing.\n",
    "request_frame = True\n",
    "while request_frame:\n",
    "    print('Getting frame: {0}'.format(n_frames))\n",
    "    s.send(bytes([1]))\n",
    "\n",
    "    # Get color\n",
    "    img.append(getMessage(s, np.uint8))\n",
    "    # Get depth\n",
    "    dep.append(getMessage(s, np.uint16))\n",
    "    # Get infrared\n",
    "    infra.append(getMessage(s, np.uint8))\n",
    "\n",
    "    print('Frame {0} done'.format(n_frames))\n",
    "    n_frames += 1\n",
    "\n",
    "    print('Type q and press enter to stop collecting frames:')\n",
    "    if input() == 'q':\n",
    "        request_frame = False\n",
    "\n",
    "print('Finished grabbing data from server.')\n",
    "s.close() # Close the connection when you're done.\n",
    "\n",
    "# Images captured from the Kinect are subsampled, meaning they are different from the ones we have on Blackboard/Canvas\n",
    "# Were they larger, the labs would be slower; but it also means that there are possible problems later when resizing.\n",
    "\n",
    "IMGW = 1920 // 2  # Image width\n",
    "IMGH = 1080 // 2 # Image Height\n",
    "DEPW = IMGW\n",
    "DEPH = IMGH\n",
    "INFRAW = DEPW\n",
    "INFRAH = DEPH\n",
    "\n",
    "img = [np.reshape(elem,(IMGH, IMGW, -1)) for elem in img]\n",
    "dep = [np.reshape(elem,(DEPH, DEPW)) for elem in dep]\n",
    "infra = [np.reshape(elem,(INFRAH, INFRAW)) for elem in infra]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... or load saved images from the disk\n",
    "\n",
    "This code will load saved images from the disk. These could be images that you have downloaded from Blackboard or Canvas, or images saved from a previous session.\n",
    "\n",
    "**Your code will throw errors relating to image size if you haven't reshaped them to their correct sizes. The images on disk are larger than the images saved from the Kinect. This is because we have subsampled the images we send over the network to speed up the labs. Make sure you use the following sizes in your reshaping code below to ensure that the images are reshaped appropriately.**\n",
    "\n",
    "Images from Blackboard:\n",
    "* Colour image size: 1920 (width) by 1080 (height) pixels\n",
    "* Depth image size: 512 (width) by 424 (height) pixels\n",
    "* Infrared image size: same as the depth image size\n",
    "\n",
    "Images saved from previously captured Kinect images:\n",
    "* Colour image size: 960 (width) by 540 (height) pixels\n",
    "* Depth image size: same as the colour image size\n",
    "* Infrared image size: same as the colour image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders for the information from disk\n",
    "img = []\n",
    "dep = []\n",
    "infra = []\n",
    "\n",
    "path_to_data = os.getcwd() + \"\\CVDL_Lab2_Data\" # Edit this if data is another directory\n",
    "data_files = glob.glob(path_to_data + \"\\KinectV2_Image_frame*.npy\")\n",
    "n_frames = np.max([int(elem[-6:-4]) for elem in data_files]) + 1\n",
    "\n",
    "for i_frame in range(n_frames):\n",
    "    print(\"Getting frame: {0}\".format(i_frame))\n",
    "    img.append(np.load(path_to_data + \"\\KinectV2_Image_frame{0:02d}.npy\".format(i_frame)))\n",
    "    dep.append(np.load(path_to_data + \"\\KinectV2_Depth_frame{0:02d}.npy\".format(i_frame)))\n",
    "    infra.append(np.load(path_to_data + \"\\KinectV2_Infra_frame{0:02d}.npy\".format(i_frame)))\n",
    "\n",
    "print(\"Finished grabbing data from disk.\")\n",
    "\n",
    "## MAKE SURE THAT YOU HAVE SET THIS APPROPRIATELY TO THE SOURCE FROM WHICH YOU LOAD YOUR IMAGES\n",
    "loading_from_saved_kinect_images = False\n",
    "\n",
    "# Reshape the frames\n",
    "if loading_from_saved_kinect_images:\n",
    "    # Images captured from the Kinect are subsampled\n",
    "    # Were they larger, the labs would be slower\n",
    "    IMGW = 1920 // 2  # Image width\n",
    "    IMGH = 1080 // 2 # Image Height\n",
    "    DEPW = IMGW\n",
    "    DEPH = IMGH\n",
    "\n",
    "else:\n",
    "    IMGW = 1920\n",
    "    IMGH = 1080\n",
    "    DEPW = 512\n",
    "    DEPH = 424\n",
    "    \n",
    "INFRAW = DEPW\n",
    "INFRAH = DEPH\n",
    "\n",
    "img = [np.reshape(elem,(IMGH, IMGW, -1)) for elem in img]\n",
    "dep = [np.reshape(elem,(DEPH, DEPW)) for elem in dep]\n",
    "infra = [np.reshape(elem,(INFRAH, INFRAW)) for elem in infra]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the new captures if needed\n",
    "Save both the numpy array information, and the appearance PNGs for later if wanted.\n",
    "\n",
    "This data will have a different size to the data we hand out on Blackboard/Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data = False\n",
    "if save_data:\n",
    "    for i_frame in range(n_frames):\n",
    "        np.save(\"KinectV2_Image_frame{0:02d}.npy\".format(i_frame), img[i_frame])\n",
    "        np.save(\"KinectV2_Depth_frame{0:02d}.npy\".format(i_frame), dep[i_frame])\n",
    "        np.save(\"KinectV2_Infra_frame{0:02d}.npy\".format(i_frame), infra[i_frame])\n",
    "        \n",
    "        im = Image.fromarray(img[i_frame][:, :, [2, 1, 0]])\n",
    "        im.save(\"KinectV2_Image_frame{0:02d}.png\".format(i_frame))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color channel correction\n",
    "Notice that the Kinect returns the color channels in BGRA order. For `matplotlib`'s image plotting functionality we will want to pass the channels in RGB order, for this we slice into our color map and reorder the channel dimension with `[:, :, [2, 1, 0]]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_frame in range(n_frames):\n",
    "    img[i_frame] = img[i_frame][:, :, [2, 1, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot frames\n",
    "In this following section we want to plot the color, depth and infrared maps we have collected from the KinectV2. \n",
    "\n",
    "Note that these maps do not match eachother exactly, pixels within the color map do not correspond directly to the corresponding pixel in the depth and infrared maps. The sensors on the KinectV2 are mounted at opposite ends of the sensor bar, and have differing resolutions and fields of view. \n",
    "\n",
    "This difference in viewpoint means that in order to align (or more formally ''register'') the color to the depth map we need to calculate the intrinsic and extrinsic parameters of the numerous Kinect sensors, we will cover this in the coming labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over our captured frames, and subplot each map \n",
    "for i_frame in range(n_frames):\n",
    "    fig = plt.figure(figsize=[9, 2])\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(img[i_frame])\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(dep[i_frame])\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(infra[i_frame])\n",
    "    plt.axis(\"off\")\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect the checkerboard pattern in an image. \n",
    "Using OpenCV, we can use the `findChessboardCorners()` method to identify a standard checkerboard pattern, commonly by identification of Harris corners. This method takes in the orignal image in grayscale, which we use `cvtColor` to achieve, and a tuple of the internal corner counts `checkerboard_pattern`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkerboard_pattern = (9, 6) # This is the internal corner count\n",
    "plot = True # toggle to control plotting of images\n",
    "\n",
    "# Detect checkerboard in each image\n",
    "img_corners = []\n",
    "infra_corners = []\n",
    "for i_frame in range(n_frames):\n",
    "    \n",
    "    # Detect corners in the RGB image\n",
    "    image = cv2.cvtColor(img[i_frame], cv2.COLOR_BGR2GRAY) # image needs to be grayscale\n",
    "    img_pattern_found, img_corner = cv2.findChessboardCorners(image, checkerboard_pattern, None) # Locate pattern\n",
    "    \n",
    "    # Detect corners in the infrared image\n",
    "    infra_pattern_found, infra_corner = cv2.findChessboardCorners(infra[i_frame], checkerboard_pattern, None) # Locate pattern\n",
    "    \n",
    "    if img_pattern_found and infra_pattern_found:\n",
    "        # Probably don't need to refine, but if needed then uncomment this section\n",
    "        #subpixel_criteria =  (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001) # Stopping criteria for refinement\n",
    "        #img_corner = cv2.cornerSubPix(image, img_corner, (11, 11), (-1, -1), subpixel_criteria)\n",
    "        #infra_corner = cv2.cornerSubPix(infra[i_frame], infra_corner, (11, 11), (-1, -1), subpixel_criteria)\n",
    "        \n",
    "        # Add detected corners into list for later\n",
    "        img_corners.append(img_corner)\n",
    "        infra_corners.append(infra_corner)\n",
    "        \n",
    "        if plot: \n",
    "            # plot corners over image\n",
    "            plt.figure()\n",
    "            plt.subplot(121)\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            corners = np.squeeze(img_corner)\n",
    "            plt.scatter(corners[:, 0], corners[:, 1], s=3)\n",
    "            plt.axis('off')\n",
    "            plt.subplot(122)\n",
    "            plt.imshow(infra[i_frame], cmap='gray')\n",
    "            corners = np.squeeze(infra_corner)\n",
    "            plt.scatter(corners[:, 0], corners[:, 1], s=3)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "n_boards = len(img_corners)\n",
    "print('Number of successfully found patterns is: {0} out of {1}'.format(n_boards, n_frames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrate cameras using detected patterns\n",
    "The following cell looks to calibrate the cameras using using the identified corners from the patterns on the calibration board in each image for each camera.\n",
    "\n",
    "First we build a coordinate system for the pattern, this requires specifying the 3D points of the corners on the board from the board's world coordinate system. For example: if we have a board of size (3, 2) we would specify corners at: \n",
    "    [[0, 0], [0, 1], [0, 2], \n",
    "     [1, 0], [1, 1], [1, 2]]\n",
    "\n",
    "Next we use OpenCV functionality to calibrate the cameras with the `calibrateCamera()` function. This returns the intrinsic camera matrix, `cam_mat`, the distortion coefficients for each camera, `distor`, and the rotation and translation vectors for each board `r_vec` and `t_vec`. We calibrate each camera seperately first, and then estimate the transform between the two systems with the `CV_CALIB_FIX_INTRINSIC` flag of `calibrateCamera()`.\n",
    "\n",
    "We then create a dictionary, `camera_ex`, containing the extrinsic parameters for each set of points found. This contains the rotation matrix and translation vector. Note here that we turn the rotation vectors `r_vec` into the rotation matrices `rot_mat` using the `Rodrigues()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the coordinate points of our checkerboard pattern (meshgrid makes this easier)\n",
    "squareH = 40\n",
    "squareW = squareH\n",
    "board = np.zeros((np.product(checkerboard_pattern), 3), dtype=np.float32)\n",
    "board[:, :2] = np.mgrid[0:squareH*(checkerboard_pattern[0] - 1):complex(checkerboard_pattern[0]),\n",
    "                                 0:squareW*(checkerboard_pattern[1] - 1):complex(checkerboard_pattern[1])].T.reshape(-1, 2)\n",
    "\n",
    "#Create a list of the checkerboard patterns\n",
    "boards = [board for i in range(n_boards)]\n",
    "\n",
    "# Calibrate camera 1 - RGB\n",
    "ret, cam_mat_rgb, distor_rgb, r_vec_rgb, t_vec_rgb = cv2.calibrateCamera(boards, img_corners, image.shape[::-1], None, None)\n",
    "# Calibrate camera 2 - Infrared\n",
    "ret, cam_mat_infra, distor_infra, r_vec_infra, t_vec_infra = cv2.calibrateCamera(boards, infra_corners, infra[0].shape[::-1], None, None)\n",
    "\n",
    "# Register the two camera systems together\n",
    "ret, cam_mat_rgb, distor_rgb, cam_mat_infra, distor_infra, R, T, E, F = \\\n",
    "    cv2.stereoCalibrate(boards, img_corners, infra_corners,\n",
    "                        cam_mat_rgb, distor_rgb,\n",
    "                        cam_mat_infra, distor_infra, \n",
    "                        image.shape[::-1], cv2.CALIB_FIX_INTRINSIC)\n",
    "\n",
    "# Set up intrinsics and extrinsic parameters\n",
    "camera_in = {}\n",
    "camera_in['rgb'] = cam_mat_rgb\n",
    "camera_in['infra'] = cam_mat_infra\n",
    "camera_ex = []\n",
    "for i_board in range(n_boards):\n",
    "    camera_ex.append({})\n",
    "    camera_ex[i_board]['rgb'] = {'rot_mat': np.asarray(cv2.Rodrigues(r_vec_rgb[i_board])[0]),\n",
    "                          'tran_mat' : np.asarray(t_vec_rgb[i_board])} \n",
    "    camera_ex[i_board]['infra'] = {'rot_mat': np.asarray(cv2.Rodrigues(r_vec_infra[i_board])[0]),\n",
    "                          'tran_mat' : np.asarray(t_vec_infra[i_board])}    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate boards to world coordinate system and plot camera-centric viewpoint\n",
    "In this cell we will transform the board coordinate system into the world coordinate system of the camera. We can then plot these points in the 3D space relative to our camera centred at (0, 0, 0). This makes the assumption that our camera is fixed and we move our checkerboard pattern around.\n",
    "\n",
    "First we create our 3D matplotlib figure and plot the camera origin. We also plot 3 lines denoting the axes for this camera system.\n",
    "\n",
    "Next we loop over our identified board patterns and translate them into the camera's world coordinate system via the rotation and translation identified via calibration.\n",
    "\n",
    "Finally, we print these new checkerboard pattern points in the 3D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open up a 3D figure\n",
    "fig3 = plt.figure(figsize=[6, 6])\n",
    "ax3 = fig3.add_subplot(111, projection='3d')\n",
    "\n",
    "# plot our infrared camera at (0, 0, 0)\n",
    "infra_cam = np.asarray([[0], [0], [0]])\n",
    "cam_axes = np.asarray([[0, 0, 0, 64, 0, 0], [0, 64, 0, 0, 0, 0], [0, 0, 0, 0, 0, 64]])\n",
    "ax3.scatter(infra_cam[0], infra_cam[1], infra_cam[2]) \n",
    "ax3.plot(cam_axes[0,:], cam_axes[1,:], cam_axes[2,:])\n",
    "\n",
    "# move our rgb camera relative to the infrared camera, and plot at R(x-T) where x is the infrared camera\n",
    "rgb_cam = (R @ (infra_cam - T))\n",
    "rgb_cam_axes = (R @ (cam_axes - T))\n",
    "ax3.scatter(rgb_cam[0], rgb_cam[1], rgb_cam[2]) \n",
    "ax3.plot(rgb_cam_axes[0,:], rgb_cam_axes[1,:], rgb_cam_axes[2,:])\n",
    "\n",
    "# Perform transforms and plot each board in the world coordinates\n",
    "for i_board in range(n_boards):\n",
    "    # Obtain the rotation matrices and translation vectors from the extrinsic parameters for each board\n",
    "    R_board = camera_ex[i_board]['rgb']['rot_mat']\n",
    "    T_board = np.squeeze(camera_ex[i_board]['rgb']['tran_mat'])\n",
    "    \n",
    "    # Perform the rotation and translation of the boards relative to the camera centred at origin: Rx+T\n",
    "    corners_world = np.matmul(R_board, board.T).T\n",
    "    corners_world += T_board.T\n",
    "    \n",
    "    # Plot our board corners in 3D\n",
    "    ax3.scatter(xs=corners_world[:, 0], ys=corners_world[:, 1], zs=corners_world[:, 2])\n",
    "\n",
    "ax3.set_xlabel('x')\n",
    "ax3.set_ylabel('y')\n",
    "ax3.set_zlabel('z')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate cameras and plot the camera-centric viewpoint\n",
    "In this cell we will transform the camera coordinate system into the world coordinate system of the board. We can then plot these points in the 3D space relative to our board centred at (0, 0, 0). This makes the assumption that our pattern is fixed and we move our camera around.\n",
    "\n",
    "First we create our 3D matplotlib figure and plot the board pattern at origin. We also plot 3 lines denoting the axes for this board system.\n",
    "\n",
    "Next we loop over each camera viewpoint in our identified board patterns and translate them into the board's world coordinate system via the inversed of the translation and rotation identified via calibration.\n",
    "\n",
    "Finally, we print these new camera positions and orientations in the 3D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of colors for our cameras\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "\n",
    "# Create coordinates for camera axes plots, these will be translated and rotated into board-centric world\n",
    "cam_axes = np.asarray([[0,  0, 0, 64, 0,  0], [0, 64, 0,  0, 0,  0], [0,  0, 0,  0, 0, 64]]).T\n",
    "\n",
    "# Open up a 3D figure\n",
    "fig3 = plt.figure(figsize=[6, 6])\n",
    "ax3 = fig3.add_subplot(111, projection='3d')\n",
    "ax3.scatter(board[:, 0], board[:, 1], board[:, 2])# plot our board at (0, 0, 0)\n",
    "ax3.plot(cam_axes[:, 0], cam_axes[:, 1], cam_axes[:, 2], 'r')\n",
    "\n",
    "# Perform transforms and plot for each camera\n",
    "for i_camera in range(n_boards): # R'(x-T)\n",
    "    \n",
    "    # Obtain the rotation matrices and translation vectors from the extrinsic parameters for each board\n",
    "    R_cam = camera_ex[i_camera]['infra']['rot_mat']\n",
    "    T_cam = np.squeeze(camera_ex[i_camera]['infra']['tran_mat'])\n",
    "    # Perform the rotation and translation of the rgb camera relative to the pattern centred at origin: R'(x-T)\n",
    "    infra_cam_axes = cam_axes - T_cam\n",
    "    infra_cam_axes = np.matmul(R_cam.T, infra_cam_axes.T).T\n",
    "    \n",
    "    # Plot infrared cameras and axes\n",
    "    ax3.scatter(xs=infra_cam_axes[:, 0], \n",
    "                ys=infra_cam_axes[:, 1], \n",
    "                zs=infra_cam_axes[:, 2], s=2)\n",
    "    ax3.plot(infra_cam_axes[:, 0], \n",
    "             infra_cam_axes[:, 1], \n",
    "             infra_cam_axes[:, 2], c=colors[i_camera % len(colors)])\n",
    "    \n",
    "    # Obtain the rotation matrices and translation vectors from the extrinsic parameters for each board\n",
    "    R_cam = camera_ex[i_camera]['rgb']['rot_mat']\n",
    "    T_cam = np.squeeze(camera_ex[i_camera]['rgb']['tran_mat'])\n",
    "    # Perform the rotation and translation of the rgb camera relative to the pattern centred at origin: R'(x-T)\n",
    "    rgb_cam_axes = cam_axes - T_cam\n",
    "    rgb_cam_axes = np.matmul(R_cam.T, rgb_cam_axes.T).T\n",
    "\n",
    "    # Plot rgb cameras and axes\n",
    "    ax3.scatter(xs=rgb_cam_axes[:, 0], \n",
    "                ys=rgb_cam_axes[:, 1], \n",
    "                zs=rgb_cam_axes[:, 2], s=2)\n",
    "    ax3.plot(rgb_cam_axes[:, 0], \n",
    "             rgb_cam_axes[:, 1], \n",
    "             rgb_cam_axes[:, 2], c=colors[i_camera % len(colors)])\n",
    "    \n",
    "ax3.set_xlabel('x')\n",
    "ax3.set_ylabel('y')\n",
    "ax3.set_zlabel('z')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing around with homography\n",
    "\n",
    "What if you have two cameras pointed at the same scene with slightly different perspectives? These two perspectives correspond to two different positions in space. You might recognise this as the *stereo system*. We can use *epipolar geometry* to calculate the position of an object or objects in two cameras.\n",
    "\n",
    "The question is phrased like this: \n",
    "\n",
    "1. Suppose I have two cameras pointed at a scene with a fixed object (imagine the chequerboard in the last lab; in this case we aren't moving it). \n",
    "2. Suppose I have identified some features in the first camera, and those corresponding features in the second camera (these features would be the corners of the chequerboard, for example). This is called *correspondence*.\n",
    "3. Then I can calculate the translation and rotation matrices by determining how much I need to translate and rotate the features in the first image to fit the features in the second image.\n",
    "\n",
    "Obviously there are some problems:\n",
    "\n",
    "1. What if the features visible in the first camera are occluded, not visible, in the second camera?\n",
    "2. What if the features in the first camera are out of view in the second camera?\n",
    "3. What if the intrinsic parameters of the two cameras are not the same? Won't the distortion cause problems? What if the distortions are non-linear? \n",
    "\n",
    "But also consider that you will need to have identified a minimum number of points in order to compute this homography.\n",
    "\n",
    "**Challenge:** What is this minimum number of points? Why is there a minimum number? What happens if we increase the number of points we select in the image?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An application of homography: panoramas\n",
    "\n",
    "Perhaps you have a smartphone. Perhaps it has a setting in the camera that lets you take panorama shots. Well, that's homography, too! Rather than many cameras on the same scene however, it's a set of overlapping images of the same scene taken from different perspectives with the same camera.\n",
    "\n",
    "Interlacing the images requires some knowledge of the scene, so we need some method to identify unique features (*viz.* SIFT).\n",
    "\n",
    "Once we know a common set of features between this chain of images, we can join them together.\n",
    "\n",
    "Let's try this with a pair of photos I took of the view outside my lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the left and right images\n",
    "left_image = plt.imread(\"left.jpg\")\n",
    "right_image = plt.imread(\"right.jpg\")\n",
    "\n",
    "# Here I rescale the images, because it's more difficult\n",
    "# to select points when the images are high resolution.\n",
    "# Feel free to comment these lines if you want to work\n",
    "# with high-resolution images.\n",
    "left_image = skimage.transform.rescale(left_image, 0.2, anti_aliasing=True, multichannel=True)\n",
    "right_image = skimage.transform.rescale(right_image, 0.2, anti_aliasing=True, multichannel=True)\n",
    "\n",
    "# Plot the left-hand image\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.title(\"Left-hand image\")\n",
    "plt.imshow(left_image)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Plot the right-hand image\n",
    "plt.subplot(122)\n",
    "plt.title(\"Right-hand image\")\n",
    "plt.imshow(right_image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the homography and mapping new points between the two images\n",
    "\n",
    "The homography is a matrix that lets us translate from a point in one image to the corresponding point in the second image.\n",
    "\n",
    "Consider a few things:\n",
    "\n",
    "* Do both images contain the same objects? Are they of the same scene?\n",
    "* Do all points in the left-hand image have points in the right-hand image?\n",
    "* Between the two images, has the camera changed its orientation with respect to the scene? Or has the camera simply translated along a line? Has it rotated?\n",
    "\n",
    "Computing the homography will tell us how the camera has changed.\n",
    "\n",
    "In order to compute the homography, we need to select three or more points on the two images. The more points we have, the stronger the correspondence.\n",
    "\n",
    "The code below does the following: \n",
    "\n",
    "* We select six points to compute the homography matrix (feel free to change this).\n",
    "* Plot the points on both images.\n",
    "* Using the points in both images, we establish the relationship between the left- and right-hand images (source and destination).\n",
    "* We reproject the source points to the destination, and plot these reprojected points, to assess the accuracy of the homography.\n",
    "    * If the homography is perfect, the red points should completely occlude the blue points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure so I can record events later.\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "# Plot the two images side by side again.\n",
    "ax[0].imshow(left_image)\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].imshow(right_image)\n",
    "ax[1].axis(\"off\")\n",
    "\n",
    "# Three points that correspond in each image.\n",
    "lefthand_points = np.array([[273., 158.],    # Feature 1: Top-right of the dark-blue book.\n",
    "                            [473., 150.],    # Feature 2: Bottom-right of the orange highlighter.\n",
    "                            [425., 355.],    # Feature 3: Top-left of the yellow book.\n",
    "                            [400., 242.],    # Feature 4: Top-right of the orange highlighter.\n",
    "                            [232., 110.],    # Feature 5: A corner of the clamp holding the research-papers.\n",
    "                            [615., 235.]]    # Feature 6: The top-left of the yellow highlighter.]   \n",
    "                          ).reshape(-1,1,2)\n",
    "righthand_points = np.array([[ 95.,  80.],   # Feature 1.\n",
    "                             [257., 155.],   # Feature 2.\n",
    "                             [ 84., 283.],   # Feature 3.\n",
    "                             [159., 202.],   # Feature 4.\n",
    "                             [144.,  51.],   # Feature 5.\n",
    "                             [332., 297.]]   # Feature 6.\n",
    "                            ).reshape(-1,1,2)\n",
    "\n",
    "# Plot these points in each image.\n",
    "ax[0].scatter(lefthand_points[...,0], lefthand_points[...,1], c=\"b\", edgecolors=\"black\")\n",
    "ax[1].scatter(righthand_points[...,0], righthand_points[...,1], c=\"b\", edgecolors=\"black\")\n",
    "\n",
    "\n",
    "# Use OpenCV to compute the homography. Look into API to see how it works.\n",
    "# `H` is our homography matrix, which transforms points from the left-hand \n",
    "# image (the source) to the right-hand image (the destimation); and `mask`\n",
    "# tells us how many matches there are between the images; i.e., how many \n",
    "# points do correspond. The mask gives 1 for each point in the source that\n",
    "# lies in the destination (the inliers) and 0 for those outside the \n",
    "# destination (the outliers).\n",
    "H, mask = cv2.findHomography(lefthand_points, righthand_points, cv2.RANSAC)\n",
    "\n",
    "# We can see how good this homography is by reprojecting the coordinates\n",
    "# of the points in the left-hand image onto the right-hand image\n",
    "reprojected_righthand_points = cv2.perspectiveTransform(lefthand_points, H)\n",
    "\n",
    "# Plot the new points in the right-hand image after the transformation in red.\n",
    "# If the projection is perfect, we would see no more blue points, as the \n",
    "# HINT: Scroll up to see the result.\n",
    "ax[1].scatter(reprojected_righthand_points[...,0], reprojected_righthand_points[...,1], c=\"r\", edgecolors=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see?\n",
    "\n",
    "Unless you have added more points, the red points should not completely overlap with every blue point.\n",
    "\n",
    "Think about why this homography is not perfect. Where do the points fail to correspond?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SIFT to find the correspondences in the two images\n",
    "\n",
    "Instead of my going through the image to find the coordinates between the two images that correspond, we can make the computer do it. It will have an easier time than me, and consequently it will find more in a far shorter amount of time.\n",
    "\n",
    "SIFT, an initialism of *scale-invariant feature transform*, finds a set of features in the image called *keypoints* and *descriptors*. What are these features, you might ask. Well search for the technique on Google and see what you find.\n",
    "\n",
    "To avoid duplicating entities beyond necessity (that's Occam's razor, by the way), we will use the SIFT implementation in OpenCV to discover these features.\n",
    "\n",
    "In the code below, we&mdash;\n",
    "\n",
    "* find features in both the left- and right-hand-side images;\n",
    "* find the set of closest matches in the two images using FLANN;\n",
    "* computing the homography from this much larger set of points;\n",
    "* visualising the correspondences (again using OpenCV); and\n",
    "* finally we return to our inital set of points to see how the source points reproject.\n",
    "\n",
    "I'm borrowing heavily from [the relevant section of the OpenCV documentation](https://docs.opencv.org/master/d1/de0/tutorial_py_feature_homography.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_image = cv2.imread(\"left.jpg\", 1)\n",
    "right_image = cv2.imread(\"right.jpg\", 1)\n",
    "\n",
    "# Here I rescale the images as I do above, except I preserve the range.\n",
    "left_image = skimage.transform.rescale(left_image, 0.2, anti_aliasing=True, multichannel=True, preserve_range=True).astype(np.uint8)\n",
    "right_image = skimage.transform.rescale(right_image, 0.2, anti_aliasing=True, multichannel=True, preserve_range=True).astype(np.uint8)\n",
    "\n",
    "# Initiate SIFT detector\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# Find the keypoints and descriptors with SIFT.\n",
    "kp1, des1 = sift.detectAndCompute(left_image, None)\n",
    "kp2, des2 = sift.detectAndCompute(right_image, None)\n",
    "\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50)\n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "# store all the good matches as per Lowe's ratio test.\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        good.append(m)\n",
    "        \n",
    "print(\"Number of good matches:\", len(good))\n",
    "\n",
    "src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "\n",
    "H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "matchesMask = mask.ravel().tolist()\n",
    "\n",
    "h,w,d = left_image.shape\n",
    "pts = np.float32([[0,0],[0,h-1],[w-1,h-1],[w-1,0]]).reshape(-1,1,2)\n",
    "dst = cv2.perspectiveTransform(pts, H)\n",
    "\n",
    "right_image_with_lines = cv2.polylines(right_image, [np.int32(dst)], True, 255, 3, cv2.LINE_AA)\n",
    "\n",
    "draw_params = dict(matchColor=(0,255,0), # draw matches in green color\n",
    "                   singlePointColor=None,\n",
    "                   matchesMask=matchesMask, # draw only inliers\n",
    "                   flags=2)\n",
    "\n",
    "img3 = cv2.drawMatches(left_image, kp1, right_image_with_lines, kp2, good, None, **draw_params)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img3, 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projecting the previous points onto the right-hand image\n",
    "\n",
    "How do the old points look here? You might notice a bit a dislocation between the two images, especially on the books. Why? Think about the perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the earlier points onto the left-hand image.\n",
    "plt.scatter(lefthand_points[...,0], lefthand_points[...,1], c=\"r\", edgecolor=\"black\")\n",
    "\n",
    "# Reproject those points onto the right-hand image.\n",
    "projected_righthand_points = cv2.perspectiveTransform(lefthand_points, H)\n",
    "\n",
    "# Plot projected points onto right-hand image.\n",
    "projected_righthand_points[...,0] += right_image.shape[1]\n",
    "plt.scatter(projected_righthand_points[...,0], projected_righthand_points[...,1], c=\"r\", edgecolor=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stitching the images together\n",
    "\n",
    "We can also use the keypoints found by SIFT to stitch the images together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we warp the left image to correspond to the right image\n",
    "joined_images = cv2.warpPerspective(left_image, H, (left_image.shape[1] + right_image.shape[1], left_image.shape[0]))\n",
    "images_before_joining = joined_images.copy()\n",
    "joined_images[0:right_image.shape[0], 0:right_image.shape[1]] = right_image\n",
    "images_before_joining[:, -right_image.shape[1]:] = right_image\n",
    "\n",
    "print(images_before_joining.copy)\n",
    "\n",
    "fig, ax = plt.subplots(2,2)\n",
    "ax[0, 0].set_title(\"Left image\")\n",
    "ax[0, 0].imshow(left_image)\n",
    "ax[0, 0].axis(\"off\")\n",
    "\n",
    "ax[0, 0].set_title(\"Right image\")\n",
    "ax[0, 1].imshow(right_image)\n",
    "ax[0, 1].axis(\"off\")\n",
    "\n",
    "ax[0, 0].set_title(\"Left image warped with the right image\")\n",
    "ax[1, 0].imshow(images_before_joining)\n",
    "ax[1, 0].axis(\"off\")\n",
    "\n",
    "ax[1, 1].set_title(\"The right image overlayed onto the position of the left image\")\n",
    "ax[1, 1].imshow(joined_images)\n",
    "ax[1, 1].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about these questions when you look at these images:\n",
    "\n",
    "* What do you notice about the left image after it has been warped?\n",
    "* Do you notice any differences between the left and right images in the scene? What consequences might this have for registering two different images? Think on different kinds of alterations to the scene that might confuse things further.\n",
    "* What differences in the positions of the two cameras have led to the mismatched registration of certain points in the two images? (Think about the depth of objects in the scene and how the perspective of the camera changes its projection and appearance on the image-plane.)\n",
    "    * How can we ensure that this mismatched registration does not affect our panoramas?\n",
    "    * If we can't prevent these mismatches, can we change the way we warp the images to account for these differences?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
